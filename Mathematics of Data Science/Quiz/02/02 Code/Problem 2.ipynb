{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) (c) (d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_tools import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\"\"\"\n",
    "Sigmoid function that takes a numpy array of any shape.\n",
    "\"\"\"\n",
    "def f(t) :\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "\"\"\"\n",
    "Forecast function which given the learned parameter vectors w and\n",
    "the data x produces the forecasts.\n",
    "\"\"\"\n",
    "def h(w,x) :\n",
    "    return f(np.dot(x,w))>0.5\n",
    "\n",
    "\"\"\"\n",
    "Computes the loss function L.\n",
    "Parameters:\n",
    "w: numpy array of length m containing the parameter vector\n",
    "X: numpy array of shape (n,m) containing n data samples as rows (each row is a data point)\n",
    "y: numpy array of length n containing the labels (0 or 1)\n",
    "Returns:\n",
    "A single float, the loss evaluated on the given arguments.\n",
    "\"\"\"\n",
    "def L(w,X,y): \n",
    "    n = len(y) \n",
    "    sum = 0 \n",
    "    for i in range(n): \n",
    "        g_i = np.dot(w, X[i]) \n",
    "        term = y[i] * np.logaddexp(0, - g_i) + (1 - y[i]) * np.logaddexp(0, g_i)  \n",
    "        sum = sum + term / n  \n",
    "    return sum  \n",
    "    \n",
    "\"\"\"\n",
    "Tests the L function\n",
    "\"\"\"\n",
    "def test_L() :\n",
    "    np.random.seed(1000)\n",
    "    v = np.array([1000])\n",
    "    w = np.random.randn(10)\n",
    "    X = np.random.randn(20,10)\n",
    "    y = np.random.randint(0,2,20)\n",
    "    L1 = L(v,v,np.array([0]))\n",
    "    L2 = L(v,v,np.array([1]))\n",
    "    L3 = L(w,X,y)\n",
    "    assert np.abs(L1-1000000) < 1e-9\n",
    "    assert np.abs(L2) < 1e-9\n",
    "    assert np.abs(L3-1.08007365415) < 1e-9\n",
    "\n",
    "\"\"\"\n",
    "Computes the gradient of the loss function.\n",
    "Parameters:\n",
    "w: numpy array of length m containing the parameter vector\n",
    "X: numpy array of shape (n,m) containing n data samples as rows (each row is a data point)\n",
    "y: numpy array of length n containing the labels (0 or 1)\n",
    "Returns:\n",
    "A numpy vector of length m containing the gradient of the \n",
    "loss evaluated on the given arguments.\n",
    "\"\"\"\n",
    "def dL(w,X,y) :\n",
    "    n = len(y) \n",
    "    sum = 0 \n",
    "    for i in range(n): \n",
    "        g_i = np.dot(w, X[i]) \n",
    "        term = X[i] * (f(g_i) - y[i]) \n",
    "        sum = sum + term / n \n",
    "    return sum \n",
    "    \n",
    "\"\"\"\n",
    "Tests the dL function\n",
    "\"\"\"\n",
    "def test_dL() :\n",
    "    np.random.seed(1000)\n",
    "    v = np.array([1000])\n",
    "    w = np.random.randn(3)\n",
    "    X = np.random.randn(200,3)\n",
    "    y = np.random.randint(0,2,200)\n",
    "    dL1 = dL(v,v,np.array([0]))\n",
    "    dL2 = dL(v,v,np.array([1]))\n",
    "    dL3 = dL(w,X,y)\n",
    "    assert np.abs(dL1-1000) < 1e-9\n",
    "    assert np.abs(dL2) < 1e-9\n",
    "    assert np.linalg.norm(dL3-np.array([-0.12669153,-0.00341384,0.02274541])) < 1e-6\n",
    "\n",
    "\"\"\"\n",
    "Runs (batch) gradient descent with a backtracking line search to minimize L.\n",
    "While typically this would include conditions/tolerances for how to stop the\n",
    "algorithm, here we only required a simplified implementation that has a given fixed \n",
    "number of steps.\n",
    "Parameters:\n",
    "w0: numpy array of length m containing the initial value of w\n",
    "X: numpy array of shape (n,m) containing the n data samples as rows\n",
    "y: numpy array of length n containing the labels (0 or 1)\n",
    "num_steps: number of gradient descent steps to run\n",
    "alpha: Armijo constant used to make sure the L function sufficiently decreases on each\n",
    "iteration\n",
    "beta: backtracking line search constant that determines how much to shrink the step\n",
    "size parameter by each time\n",
    "Returns: the tuple w,ws where\n",
    "w: numpy array of length m containing the final value of w\n",
    "ws: a python list of num_steps numpy arrays of length m containing the w-values computed\n",
    "at each iteration\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(w0,X,y,num_steps=200,alpha=0.01,beta=0.5) :\n",
    "    w_s = []  \n",
    "    w = w0 \n",
    "    for i in range(num_steps): \n",
    "        t = 1 \n",
    "        while (L(w, X, y) - L(w - t * dL(w, X, y), X, y) - alpha * t * np.dot(dL(w, X, y), dL(w, X, y)) < 0 ): \n",
    "            t = t * beta \n",
    "        print(\"t = \", t) \n",
    "        w = w - t * dL(w, X, y) \n",
    "        w_s.append(w) \n",
    "    return w, w_s \n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Standarizes the training and test data using the training data to compute\n",
    "the mean and standard deviation.\n",
    "\"\"\"\n",
    "def standardize(train,test) :\n",
    "    m = np.mean(train,axis=0)\n",
    "    std = np.std(train,axis=0)\n",
    "    std[np.abs(std)<1e-9] = 1\n",
    "    return (train-m)/std,(test-m)/std,m,std\n",
    "\n",
    "\"\"\"\n",
    "Runs the optimization and creates the plots\n",
    "\"\"\"\n",
    "def run(name,fun,train_x,train_y,test_x,test_y,mean,std) :\n",
    "    t = time.time()\n",
    "    g_w,g_ws = fun(np.zeros(train_x.shape[1]),train_x,train_y)\n",
    "    print('%s Time = %fs'%(name,time.time()-t))\n",
    "    print('%s Training Loss = %f'%(name,L(g_w,train_x,train_y)))\n",
    "    test_err = np.sum(np.abs(h(g_w,test_x)-test_y))*1.0/test_x.shape[0]\n",
    "    print('%s Test Error = %f'%(name,test_err))\n",
    "    ls = [L(w,train_x,train_y) for w in g_ws]\n",
    "    tls = [L(w,test_x,test_y) for w in g_ws]\n",
    "    terr = [np.sum(np.abs(h(w,test_x)-test_y))/test_x.shape[0] for w in g_ws]\n",
    "    plt.plot(ls)\n",
    "    plt.title('%s Training Loss'%name)\n",
    "    plt.savefig('%s_Train_Loss.pdf'%name,bbox_inches='tight')\n",
    "    plt.close()\n",
    "    plt.title('%s Test Loss'%name)\n",
    "    plt.plot(tls)\n",
    "    plt.savefig('%s_Test_Loss.pdf'%name,bbox_inches='tight')\n",
    "    plt.close()\n",
    "    plt.plot(terr)\n",
    "    plt.title('%s Test Error'%name)\n",
    "    plt.savefig('%s_Test_Error.pdf'%name,bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "def main() :\n",
    "    test_L() \n",
    "    test_dL() \n",
    "    \n",
    "    train = load_train_data(\"mnist_all.mat\") \n",
    "    test = load_test_data(\"mnist_all.mat\") \n",
    "    print('Using %d training examples and %d test examples'%(train.shape[0],test.shape[0])) \n",
    "    #We will determine if the image is a '5' or not \n",
    "    train[:,-1] = train[:,-1]==5\n",
    "    test[:,-1] = test[:,-1]==5\n",
    "    train_x,train_y = train[:,:-1],train[:,-1]\n",
    "    test_x,test_y = test[:,:-1],test[:,-1]\n",
    "    train_x,test_x,mean,std = standardize(train_x,test_x)\n",
    "\n",
    "    run('GD',gradient_descent,train_x,train_y,test_x,test_y,mean,std)\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afdc64321e0bbd822c33b0b4adbff48972901a3387c9fb41a2bea816eca0a25f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
