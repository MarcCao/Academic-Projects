{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of stocks = 18, # of days = 433\n",
      "[[ -2.550804  -3.200012   0.239204 ...   0.26794   -0.239998  -0.32    ]\n",
      " [ -1.942085  -1.139954  -0.956821 ...  -1.63744    2.079994  -0.55    ]\n",
      " [ -4.106422 -24.710022  -1.798817 ...  -2.828319   4.040001  -0.21    ]\n",
      " ...\n",
      " [ -1.210007 -12.599976  -0.149994 ...   0.369995  -0.159999  -0.03    ]\n",
      " [  0.059998  -4.330017   0.279998 ...   0.139999  -0.090001  -0.06    ]\n",
      " [ -2.659989   3.350037  -0.5      ...   0.080002  -0.039997   0.13    ]]\n",
      "[200. 200. 200. 200. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100.]\n",
      "4309.949525975801\n",
      "      aapl      amzn      msft      goog       xom      apc       cvx  \\\n",
      "  0.125647  0.778287  0.052098  0.439213  0.018077 -0.00924  0.078222   \n",
      "\n",
      "         c        gs       jpm       aet       jnj       dgx       spy  \\\n",
      "  0.049329  0.131921  0.077361  0.110505  0.086558  0.077386  0.129704   \n",
      "\n",
      "       xlf       sso      sds       uso  \n",
      "  0.026155  0.081656 -0.08088 -0.001713  \n",
      "356.55335694444454\n",
      "0.12564666203703703\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from statistics import variance\n",
    "\"\"\"\n",
    "Loads financial data as a pandas dataframe\n",
    "\"\"\"\n",
    "def load_dataframe(filename) :\n",
    "    return pd.read_csv(filename,index_col=0)\n",
    "\n",
    "\"\"\"\n",
    "Loads financial data as a tuple: names,data.  \n",
    "names is a list of the stock names represented in each column.\n",
    "data is a 2d numpy array.  Each row of data corresponds to a trading day.\n",
    "data[i,j] is the price (technically the adjusted closing price) of \n",
    "instrument names[j] on the ith day.  The days are ordered chronologically.\n",
    "\"\"\"\n",
    "def load_data(filename) :\n",
    "    df = pd.read_csv(filename,index_col=0)\n",
    "    names = df.columns.values.tolist()\n",
    "    data = df.to_numpy()\n",
    "    #data = df.as_matrix()\n",
    "    return names,data\n",
    "\n",
    "\"\"\"\n",
    "Given a 1d numpy array vec of n values, and a list of n names,\n",
    "prints the values and their associated names.\n",
    "\"\"\"\n",
    "def pretty_print(vec,names) :\n",
    "    print(pd.DataFrame(vec,names,['']).transpose())\n",
    "\n",
    "\"\"\"\n",
    "Given a 1d numpy array vec of n values, and a list of n names,\n",
    "prints the values and their associated names in a LaTeX friendly\n",
    "format.\n",
    "\"\"\"\n",
    "def pretty_print_latex(vec,names,num_col=6) :\n",
    "    print(\"\\\\begin{center}\")\n",
    "    print(\"\\\\begin{tabular}{c\"+(\"|c\"*(num_col-1))+\"}\")\n",
    "    for i in range(0,len(names),num_col) :\n",
    "        start = True\n",
    "        for j in range(i,min(i+num_col,len(names))) :\n",
    "            if not start :\n",
    "                print(\" & \",end='')\n",
    "            start = False\n",
    "            print(names[j],end='')\n",
    "        print(\"\\\\\\\\\")\n",
    "        start = True\n",
    "        for j in range(i,min(i+num_col,len(names))) :\n",
    "            if not start :\n",
    "                print(\" & \",end='')\n",
    "            start = False\n",
    "            print(\"%.04f\"%vec[j],end='')\n",
    "        print(\"\\\\\\\\\")\n",
    "        if i+num_col < len(names) :\n",
    "            print(\"\\\\hline\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\end{center}\")\n",
    "\n",
    "def main() :\n",
    "    names,data = load_data('stockprices.csv')\n",
    "    print(\"# of stocks = %d, # of days = %d\"%(data.shape[1],data.shape[0]))\n",
    "    # center the data\n",
    "    for i in range(0,18):\n",
    "        data[:,i] = data[:,i] - data[:,i].mean()\n",
    "    #pretty_print(data[0,:],names)\n",
    "\n",
    "    ### Part (a)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(data)\n",
    "\n",
    "    c1 = pca.components_[0]\n",
    "    #print(\"component 1:\")\n",
    "    #pretty_print(c1, names)\n",
    "    c2 = pca.components_[1]\n",
    "    #print(\"component 2:\")\n",
    "    #pretty_print(c2, names)\n",
    "\n",
    "    v_a = variance(data[:,1])\n",
    "    #print(\"variance of Amazon is:\", v_a)\n",
    "    v_g = variance(data[:,3])\n",
    "    #print(\"variance of Google is:\", v_g)\n",
    "\n",
    "    # calculating variance for each stock\n",
    "    v1 = np.zeros((18))\n",
    "    for i in range(0,18):\n",
    "        v1[i] = variance(data[:,i])\n",
    "    #print(\"variance of each stock:\")\n",
    "    #pretty_print(v1,names)\n",
    "\n",
    "\n",
    "    ### Part (b)\n",
    "    # scaling data\n",
    "    scaler = preprocessing.StandardScaler().fit(data)\n",
    "    data_scaled = scaler.transform(data)\n",
    "\n",
    "    # scaled PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(data_scaled)\n",
    "\n",
    "    c1 = pca.components_[0]\n",
    "    #print(\"component 1:\")\n",
    "    #pretty_print(c1, names)\n",
    "    c2 = pca.components_[1]\n",
    "    #print(\"component 2:\")\n",
    "    #pretty_print(c2, names)\n",
    "\n",
    "    ### Part (c)\n",
    "    # calculate the variance of daily returns\n",
    "    R = np.diff(data.T)\n",
    "    R = R.T\n",
    "    print(R)\n",
    "\n",
    "    A = 100*np.ones((18))\n",
    "    A[0:4] = 200\n",
    "    print(A)\n",
    "    sig = np.cov(R.T)\n",
    "    var = np.dot(A.T, np.dot(sig, A))\n",
    "    print(np.sqrt(var))\n",
    "\n",
    "    ### Part (d)\n",
    "    # calculating the mean of y = Ax\n",
    "    m = np.zeros((18))\n",
    "    for i in range(0,18):\n",
    "        m[i] = R[:,i].mean()\n",
    "    #print(\"mean of daily returns:\")\n",
    "    pretty_print(m,names)\n",
    "    \n",
    "    print(np.dot(A,m))\n",
    "    print(R[:,0].mean())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()\n",
    "\n",
    "\n",
    "#pretty_print_latex(data[0,:],names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "heights_weights.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23556/1071546624.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m### CHANGE PATH TO WHEREVER YOU SAVE THE DATA FILES ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"heights_weights.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#n_train = 20000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mt:\\Software\\Anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1066\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mt:\\Software\\Anaconda\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mt:\\Software\\Anaconda\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    529\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: heights_weights.txt not found."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "import os.path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "plt.close(\"all\")\n",
    "plt.rcParams['xtick.major.pad']='8'\n",
    "plt.rcParams['ytick.major.pad']='8'\n",
    "\n",
    "### CHANGE PATH TO WHEREVER YOU SAVE THE DATA FILES ### \n",
    "data = np.loadtxt(\"heights_weights.txt\") \n",
    "\n",
    "#n_train = 20000\n",
    "n_train = 100\n",
    "n_test = 1000\n",
    "\n",
    "# Randomly select training, test and validation sets\n",
    "np.random.shuffle(data)\n",
    "data_train = data[range(n_train),:]\n",
    "data_test = data[range(n_train, n_train+n_test),:]\n",
    "heights_train = data_train[:,1]\n",
    "weights_train = data_train[:,2]\n",
    "heights_test = data_test[:,1]\n",
    "weights_test = data_test[:,2]\n",
    "\n",
    "### INSERT CODE HERE ###\n",
    "# Hint: To fit model 1 use the expression you found directly\n",
    "# to fit model 2 you can use np.linalg.lstsq\n",
    "x0 = 1/np.dot(heights_train.T, heights_train)\n",
    "x = np.dot(np.dot(x0,heights_train.T), weights_train)\n",
    "pred_model_1 = np.dot(x,heights_test)\n",
    "A = np.vstack([heights_train, np.ones(len(heights_train))]).T\n",
    "y1, y0 = np.linalg.lstsq(A, weights_train, rcond=None)[0]\n",
    "pred_model_2 = np.dot(y1, heights_test) + y0\n",
    "\n",
    "plt.figure(figsize=(12, 9))  \n",
    "plt.plot(heights_test, weights_test, '.',  color='skyblue',markeredgecolor='blue')\n",
    "plt.plot(heights_test, pred_model_1,  c=\"red\")\n",
    "\n",
    "plt.figure(figsize=(12, 9))  \n",
    "plt.plot(heights_test, weights_test, '.', color='skyblue',markeredgecolor='blue')\n",
    "plt.plot(heights_test, pred_model_2, c=\"red\")\n",
    "\n",
    "error_1 = np.sum(np.abs((pred_model_1 - weights_test)/weights_test))/n_test\n",
    "error_2 = np.sum(np.abs((pred_model_2 - weights_test)/weights_test))/n_test\n",
    "\n",
    "print (\"Model 1 relative error: \" + str(error_1))\n",
    "print (\"Model 2 relative error: \" + str(error_2))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afdc64321e0bbd822c33b0b4adbff48972901a3387c9fb41a2bea816eca0a25f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
